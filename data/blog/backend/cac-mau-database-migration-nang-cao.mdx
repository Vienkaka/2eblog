---
title: 'Migrations | 4. Các mẫu Database Migration nâng cao'
thumnail: 'https://fastly.picsum.photos/id/688/4608/3072.jpg?hmac=qIpK7qUXym0mfdFosdvte_Ia2COVGw9-WjBicJ3eSHg'
date: '2025-10-21'
lastmod: '2025-10-21'
tags: ['database', 'migration', 'backend', 'devops', 'production', 'advanced', 'patterns']
draft: false
summary: 'Khám phá các advanced patterns và strategies cho database migration trong production environment, bao gồm zero-downtime deployments, data consistency, và enterprise-level practices.'
images:
  [
    'https://fastly.picsum.photos/id/688/4608/3072.jpg?hmac=qIpK7qUXym0mfdFosdvte_Ia2COVGw9-WjBicJ3eSHg',
  ]
authors: ['default']
layout: PostLayout
canonicalUrl: https://gaoden.vercel.app/blog/backend/database-migration-advanced-patterns-production-ready
---

## Giới thiệu

Trong môi trường production, database migration không chỉ đơn giản là thay đổi cấu trúc database. Đây là một quá trình phức tạp đòi hỏi sự cẩn thận, planning kỹ lưỡng và áp dụng các advanced patterns để đảm bảo:

- **Zero-downtime deployments**
- **Data consistency**
- **Rollback capabilities**
- **Performance optimization**
- **Enterprise-level reliability**

Bài viết này sẽ đi sâu vào các advanced patterns và production-ready strategies cho database migration.

## Advanced Migration Patterns

### 1. **Expand-Contract Pattern (Strangler Fig Pattern)**

Pattern này cho phép thay đổi cấu trúc database mà không làm gián đoạn service:

```sql
-- Phase 1: EXPAND - Thêm cấu trúc mới song song với cũ
-- Migration: 001_add_new_schema.sql
CREATE TABLE users_v2 (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    -- New fields
    first_name VARCHAR(50),
    last_name VARCHAR(50),
    phone VARCHAR(20),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Copy existing data
INSERT INTO users_v2 (id, username, email, first_name, last_name, phone)
SELECT id, username, email,
       SPLIT_PART(full_name, ' ', 1),
       SPLIT_PART(full_name, ' ', 2),
       phone
FROM users;
```

```sql
-- Phase 2: CONTRACT - Chuyển đổi hoàn toàn sang schema mới
-- Migration: 002_migrate_to_new_schema.sql
BEGIN;

-- Update application to use users_v2
-- (Application code changes)

-- Drop old table after verification
-- DROP TABLE users;
-- ALTER TABLE users_v2 RENAME TO users;

COMMIT;
```

### 2. **Blue-Green Database Pattern**

Triển khai song song hai phiên bản database:

```yaml
# docker-compose.yml
version: '3.8'
services:
  app-blue:
    image: myapp:latest
    environment:
      - DB_HOST=db-blue
      - DB_COLOR=blue
    depends_on:
      - db-blue

  app-green:
    image: myapp:latest
    environment:
      - DB_HOST=db-green
      - DB_COLOR=green
    depends_on:
      - db-green

  db-blue:
    image: postgres:13
    environment:
      POSTGRES_DB: myapp_blue
    volumes:
      - blue_data:/var/lib/postgresql/data

  db-green:
    image: postgres:13
    environment:
      POSTGRES_DB: myapp_green
    volumes:
      - green_data:/var/lib/postgresql/data
```

```sql
-- Migration script for blue-green
-- 1. Run migration on green database
-- 2. Sync data from blue to green
-- 3. Switch traffic to green
-- 4. Blue becomes new green for next deployment
```

### 3. **Feature Flag-Driven Migrations**

Sử dụng feature flags để kiểm soát migration:

```javascript
// migration-controller.js
class MigrationController {
  async migrateWithFeatureFlag(migrationName, featureFlag) {
    const isEnabled = await this.featureFlagService.isEnabled(featureFlag)

    if (isEnabled) {
      await this.runMigration(migrationName)
    } else {
      console.log(`Migration ${migrationName} skipped - feature flag disabled`)
    }
  }

  async runMigration(migrationName) {
    try {
      await this.db.transaction(async (trx) => {
        await this.executeMigration(migrationName, trx)
        await this.logMigrationStatus(migrationName, 'success')
      })
    } catch (error) {
      await this.logMigrationStatus(migrationName, 'failed', error.message)
      throw error
    }
  }
}
```

```sql
-- Migration với feature flag support
-- 001_add_advanced_user_features.sql
DO $$
BEGIN
  -- Check if feature is enabled
  IF EXISTS (SELECT 1 FROM feature_flags WHERE name = 'advanced_user_features' AND enabled = true) THEN
    -- Add new columns
    ALTER TABLE users ADD COLUMN IF NOT EXISTS profile_picture_url VARCHAR(255);
    ALTER TABLE users ADD COLUMN IF NOT EXISTS bio TEXT;
    ALTER TABLE users ADD COLUMN IF NOT EXISTS social_links JSONB;

    -- Create new indexes
    CREATE INDEX IF NOT EXISTS idx_users_profile_picture ON users(profile_picture_url);
    CREATE INDEX IF NOT EXISTS idx_users_social_links ON users USING GIN(social_links);
  END IF;
END $$;
```

## Zero-Downtime Migration Strategies

### 1. **Online Schema Changes**

Sử dụng các kỹ thuật để thay đổi schema mà không lock table:

```sql
-- PostgreSQL: Thêm cột với giá trị default
-- Bước 1: Thêm cột nullable
ALTER TABLE large_table ADD COLUMN new_field VARCHAR(100);

-- Bước 2: Populate dữ liệu trong background (batch processing)
UPDATE large_table
SET new_field = 'default_value'
WHERE id BETWEEN 1 AND 10000
  AND new_field IS NULL;

-- Bước 3: Thêm constraint sau khi populate xong
ALTER TABLE large_table ALTER COLUMN new_field SET NOT NULL;
```

```sql
-- MySQL: Online DDL với pt-online-schema-change
-- Thay đổi cột mà không lock table
ALTER TABLE users
MODIFY COLUMN email VARCHAR(255) NOT NULL,
ALGORITHM=INPLACE, LOCK=NONE;
```

### 2. **Chunked Data Migration**

Xử lý dữ liệu lớn theo từng chunk:

```javascript
// chunked-migration.js
class ChunkedMigration {
  constructor(db, chunkSize = 1000) {
    this.db = db
    this.chunkSize = chunkSize
  }

  async migrateLargeTable(tableName, migrationFn) {
    let offset = 0
    let hasMore = true

    while (hasMore) {
      const rows = await this.db.query(`
        SELECT * FROM ${tableName} 
        ORDER BY id 
        LIMIT ${this.chunkSize} OFFSET ${offset}
      `)

      if (rows.length === 0) {
        hasMore = false
        break
      }

      await this.db.transaction(async (trx) => {
        for (const row of rows) {
          await migrationFn(row, trx)
        }
      })

      offset += this.chunkSize

      // Log progress
      console.log(`Processed ${offset} rows`)

      // Small delay to prevent overwhelming the database
      await this.sleep(100)
    }
  }

  sleep(ms) {
    return new Promise((resolve) => setTimeout(resolve, ms))
  }
}
```

```sql
-- Example: Migrate user data in chunks
-- 001_migrate_user_preferences.sql
DO $$
DECLARE
  batch_size INTEGER := 1000;
  processed INTEGER := 0;
  total_rows INTEGER;
BEGIN
  -- Get total count
  SELECT COUNT(*) INTO total_rows FROM users;

  -- Process in batches
  WHILE processed < total_rows LOOP
    -- Update batch
    UPDATE users
    SET preferences = '{}'::jsonb
    WHERE id IN (
      SELECT id FROM users
      WHERE preferences IS NULL
      LIMIT batch_size
    );

    processed := processed + batch_size;

    -- Log progress
    RAISE NOTICE 'Processed % of % rows', processed, total_rows;

    -- Small delay
    PERFORM pg_sleep(0.1);
  END LOOP;
END $$;
```

### 3. **Read Replica Migration**

Sử dụng read replica để giảm tải:

```sql
-- Setup read replica
-- 1. Create read replica
-- 2. Run migration on replica
-- 3. Promote replica to master
-- 4. Update application connection

-- Migration script
-- 001_migrate_with_read_replica.sql
DO $$
BEGIN
  -- Check if we're on read replica
  IF NOT pg_is_in_recovery() THEN
    RAISE EXCEPTION 'This migration should run on read replica first';
  END IF;

  -- Run migration
  ALTER TABLE users ADD COLUMN new_field VARCHAR(100);
  UPDATE users SET new_field = 'migrated_value';

  -- Log completion
  INSERT INTO migration_log (migration_name, status, executed_at)
  VALUES ('001_migrate_with_read_replica', 'completed', NOW());
END $$;
```

## Data Consistency Patterns

### 1. **Event Sourcing với Migration**

Sử dụng event sourcing để đảm bảo data consistency:

```javascript
// event-sourced-migration.js
class EventSourcedMigration {
  constructor(eventStore, projectionStore) {
    this.eventStore = eventStore
    this.projectionStore = projectionStore
  }

  async migrateWithEventSourcing() {
    // 1. Read all events
    const events = await this.eventStore.getAllEvents()

    // 2. Rebuild projections with new schema
    for (const event of events) {
      await this.replayEvent(event)
    }

    // 3. Verify consistency
    await this.verifyDataConsistency()
  }

  async replayEvent(event) {
    // Replay event with new projection logic
    const projection = await this.buildProjection(event)
    await this.projectionStore.save(projection)
  }
}
```

### 2. **Two-Phase Commit Pattern**

```sql
-- Two-phase commit for complex migrations
-- Phase 1: Prepare
BEGIN;
  -- Validate all conditions
  SELECT COUNT(*) FROM users WHERE email IS NULL;
  -- Should return 0

  -- Prepare changes
  ALTER TABLE users ADD COLUMN email_verified BOOLEAN DEFAULT FALSE;

  -- Log preparation
  INSERT INTO migration_prepare_log (migration_id, status, prepared_at)
  VALUES ('002_verify_emails', 'prepared', NOW());
COMMIT;

-- Phase 2: Commit (or Rollback)
BEGIN;
  -- Apply changes
  UPDATE users SET email_verified = true WHERE email IS NOT NULL;

  -- Log completion
  UPDATE migration_prepare_log
  SET status = 'committed', committed_at = NOW()
  WHERE migration_id = '002_verify_emails';
COMMIT;
```

### 3. **Compensating Transactions**

```sql
-- Compensating transaction pattern
-- Main migration
BEGIN;
  -- Add new column
  ALTER TABLE orders ADD COLUMN total_tax DECIMAL(10,2);

  -- Calculate and populate tax
  UPDATE orders SET total_tax = total_amount * 0.1;

  -- Log the change
  INSERT INTO migration_log (migration_name, status, executed_at)
  VALUES ('003_add_tax_calculation', 'completed', NOW());
COMMIT;

-- Compensating transaction (if rollback needed)
BEGIN;
  -- Remove the column
  ALTER TABLE orders DROP COLUMN total_tax;

  -- Log the rollback
  UPDATE migration_log
  SET status = 'rolled_back', rolled_back_at = NOW()
  WHERE migration_name = '003_add_tax_calculation';
COMMIT;
```

## Performance Optimization

### 1. **Parallel Migration Execution**

```javascript
// parallel-migration.js
class ParallelMigration {
  constructor(db, maxConcurrency = 3) {
    this.db = db
    this.maxConcurrency = maxConcurrency
  }

  async executeParallelMigrations(migrations) {
    const semaphore = new Semaphore(this.maxConcurrency)

    const promises = migrations.map(async (migration) => {
      await semaphore.acquire()
      try {
        return await this.executeMigration(migration)
      } finally {
        semaphore.release()
      }
    })

    const results = await Promise.allSettled(promises)
    return this.processResults(results)
  }

  async executeMigration(migration) {
    const startTime = Date.now()

    try {
      await this.db.query(migration.sql)
      const duration = Date.now() - startTime

      return {
        name: migration.name,
        status: 'success',
        duration,
      }
    } catch (error) {
      return {
        name: migration.name,
        status: 'failed',
        error: error.message,
      }
    }
  }
}
```

### 2. **Index Management During Migration**

```sql
-- Smart index management
-- 001_optimize_indexes_during_migration.sql
DO $$
BEGIN
  -- Drop indexes before migration
  DROP INDEX IF EXISTS idx_users_email;
  DROP INDEX IF EXISTS idx_users_username;

  -- Run migration
  ALTER TABLE users ADD COLUMN full_name VARCHAR(100);
  UPDATE users SET full_name = first_name || ' ' || last_name;

  -- Recreate indexes with new structure
  CREATE INDEX idx_users_email ON users(email);
  CREATE INDEX idx_users_username ON users(username);
  CREATE INDEX idx_users_full_name ON users(full_name);

  -- Analyze table for query planner
  ANALYZE users;
END $$;
```

### 3. **Connection Pooling cho Migration**

```javascript
// migration-with-pooling.js
const { Pool } = require('pg')

class MigrationWithPooling {
  constructor(config) {
    this.pool = new Pool({
      ...config,
      max: 20, // Maximum connections
      idleTimeoutMillis: 30000,
      connectionTimeoutMillis: 2000,
    })
  }

  async runMigrationWithPooling(migration) {
    const client = await this.pool.connect()

    try {
      await client.query('BEGIN')
      await client.query(migration.sql)
      await client.query('COMMIT')
    } catch (error) {
      await client.query('ROLLBACK')
      throw error
    } finally {
      client.release()
    }
  }
}
```

## Monitoring và Observability

### 1. **Migration Metrics**

```sql
-- Migration metrics table
CREATE TABLE migration_metrics (
    id SERIAL PRIMARY KEY,
    migration_name VARCHAR(255) NOT NULL,
    start_time TIMESTAMP NOT NULL,
    end_time TIMESTAMP,
    duration_ms INTEGER,
    rows_affected INTEGER,
    status VARCHAR(20) NOT NULL,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Function to track migration metrics
CREATE OR REPLACE FUNCTION track_migration_metrics(
    p_migration_name VARCHAR(255),
    p_start_time TIMESTAMP,
    p_end_time TIMESTAMP,
    p_rows_affected INTEGER,
    p_status VARCHAR(20),
    p_error_message TEXT DEFAULT NULL
) RETURNS VOID AS $$
BEGIN
    INSERT INTO migration_metrics (
        migration_name, start_time, end_time,
        duration_ms, rows_affected, status, error_message
    ) VALUES (
        p_migration_name, p_start_time, p_end_time,
        EXTRACT(EPOCH FROM (p_end_time - p_start_time)) * 1000,
        p_rows_affected, p_status, p_error_message
    );
END;
$$ LANGUAGE plpgsql;
```

### 2. **Real-time Migration Monitoring**

```javascript
// migration-monitor.js
class MigrationMonitor {
  constructor(db, metricsCollector) {
    this.db = db
    this.metricsCollector = metricsCollector
  }

  async monitorMigration(migrationName, migrationFn) {
    const startTime = Date.now()
    let rowsAffected = 0

    try {
      // Start monitoring
      await this.startMonitoring(migrationName)

      // Execute migration
      rowsAffected = await migrationFn()

      // Record success
      await this.recordSuccess(migrationName, startTime, rowsAffected)
    } catch (error) {
      // Record failure
      await this.recordFailure(migrationName, startTime, error)
      throw error
    }
  }

  async startMonitoring(migrationName) {
    // Log to monitoring system
    console.log(`Starting migration: ${migrationName}`)

    // Set up alerts
    this.setupAlerts(migrationName)
  }

  setupAlerts(migrationName) {
    // Set up timeout alert
    setTimeout(() => {
      console.warn(`Migration ${migrationName} is taking longer than expected`)
    }, 300000) // 5 minutes
  }
}
```

### 3. **Health Checks During Migration**

```sql
-- Health check functions
CREATE OR REPLACE FUNCTION check_migration_health()
RETURNS TABLE (
    check_name VARCHAR(100),
    status VARCHAR(20),
    message TEXT
) AS $$
BEGIN
    -- Check for long-running queries
    RETURN QUERY
    SELECT
        'long_running_queries'::VARCHAR(100) as check_name,
        CASE
            WHEN COUNT(*) > 0 THEN 'warning'::VARCHAR(20)
            ELSE 'ok'::VARCHAR(20)
        END as status,
        'Found ' || COUNT(*) || ' long-running queries' as message
    FROM pg_stat_activity
    WHERE state = 'active'
      AND query_start < NOW() - INTERVAL '5 minutes';

    -- Check for locks
    RETURN QUERY
    SELECT
        'table_locks'::VARCHAR(100) as check_name,
        CASE
            WHEN COUNT(*) > 0 THEN 'warning'::VARCHAR(20)
            ELSE 'ok'::VARCHAR(20)
        END as status,
        'Found ' || COUNT(*) || ' table locks' as message
    FROM pg_locks
    WHERE mode = 'ExclusiveLock';

    -- Check disk space
    RETURN QUERY
    SELECT
        'disk_space'::VARCHAR(100) as check_name,
        CASE
            WHEN pg_database_size(current_database()) > 1000000000 THEN 'warning'::VARCHAR(20)
            ELSE 'ok'::VARCHAR(20)
        END as status,
        'Database size: ' || pg_size_pretty(pg_database_size(current_database())) as message;
END;
$$ LANGUAGE plpgsql;
```

## Enterprise-Level Migration Strategies

### 1. **Multi-Region Migration**

```yaml
# multi-region-migration.yml
migration:
  strategy: 'multi-region'
  regions:
    - name: 'us-east-1'
      primary: true
      migration_order: 1
    - name: 'us-west-2'
      primary: false
      migration_order: 2
    - name: 'eu-west-1'
      primary: false
      migration_order: 3

  steps:
    - name: 'prepare'
      description: 'Prepare migration scripts'
    - name: 'validate'
      description: 'Validate migration on staging'
    - name: 'deploy'
      description: 'Deploy to regions in order'
    - name: 'verify'
      description: 'Verify data consistency'
```

### 2. **Compliance và Audit Trail**

```sql
-- Audit trail for migrations
CREATE TABLE migration_audit (
    id SERIAL PRIMARY KEY,
    migration_name VARCHAR(255) NOT NULL,
    executed_by VARCHAR(100) NOT NULL,
    executed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    environment VARCHAR(50) NOT NULL,
    rollback_script TEXT,
    approval_id VARCHAR(100),
    compliance_checks JSONB
);

-- Function to log migration with compliance
CREATE OR REPLACE FUNCTION log_migration_with_compliance(
    p_migration_name VARCHAR(255),
    p_executed_by VARCHAR(100),
    p_environment VARCHAR(50),
    p_rollback_script TEXT,
    p_approval_id VARCHAR(100)
) RETURNS VOID AS $$
BEGIN
    INSERT INTO migration_audit (
        migration_name, executed_by, environment,
        rollback_script, approval_id, compliance_checks
    ) VALUES (
        p_migration_name, p_executed_by, p_environment,
        p_rollback_script, p_approval_id,
        jsonb_build_object(
            'gdpr_compliant', true,
            'sox_compliant', true,
            'data_retention_checked', true
        )
    );
END;
$$ LANGUAGE plpgsql;
```

### 3. **Automated Rollback System**

```javascript
// automated-rollback.js
class AutomatedRollbackSystem {
  constructor(db, monitoringService) {
    this.db = db
    this.monitoringService = monitoringService
  }

  async setupAutomatedRollback(migrationName, rollbackScript) {
    // Set up monitoring
    const monitor = await this.monitoringService.createMonitor({
      name: `migration-${migrationName}`,
      metrics: ['error_rate', 'response_time', 'data_consistency'],
      thresholds: {
        error_rate: 0.05, // 5%
        response_time: 1000, // 1 second
        data_consistency: 0.99, // 99%
      },
    })

    // Set up automatic rollback
    monitor.on('threshold_exceeded', async (metric, value) => {
      console.error(`Metric ${metric} exceeded threshold: ${value}`)
      await this.executeRollback(migrationName, rollbackScript)
    })
  }

  async executeRollback(migrationName, rollbackScript) {
    try {
      console.log(`Executing rollback for migration: ${migrationName}`)
      await this.db.query(rollbackScript)

      // Log rollback
      await this.logRollback(migrationName, 'success')
    } catch (error) {
      console.error(`Rollback failed for ${migrationName}:`, error)
      await this.logRollback(migrationName, 'failed', error.message)
    }
  }
}
```

## Kết luận

Advanced database migration patterns và production-ready strategies đòi hỏi:

- **Comprehensive planning** và risk assessment
- **Advanced patterns** như Expand-Contract, Blue-Green, Feature Flags
- **Zero-downtime strategies** với online schema changes
- **Data consistency** với event sourcing và compensating transactions
- **Performance optimization** với parallel execution và smart indexing
- **Enterprise-level monitoring** và automated rollback

Việc áp dụng đúng các patterns này sẽ giúp:

- **Giảm thiểu rủi ro** trong production environment
- **Đảm bảo data consistency** và system reliability
- **Tối ưu hóa performance** và user experience
- **Hỗ trợ enterprise requirements** về compliance và audit

Bạn có kinh nghiệm gì về advanced migration patterns không? Hãy chia sẻ trong phần comment nhé!

## Tài liệu tham khảo

- [Database Migration Patterns](https://martinfowler.com/articles/evodb.html)
- [Zero-Downtime Database Migrations](https://www.postgresql.org/docs/current/ddl-alter.html)
- [Event Sourcing Patterns](https://microservices.io/patterns/data/event-sourcing.html)
- [Blue-Green Deployment](https://martinfowler.com/bliki/BlueGreenDeployment.html)
- [Feature Flags Best Practices](https://docs.microsoft.com/en-us/azure/azure-app-configuration/concept-feature-management)
